{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Random Cut Forests - Anomoly Detection\n",
    "\n",
    "***Unsupervised anomaly detection on timeseries data a Random Cut Forest algorithm.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href='https://github.com/bsnively/sagemaker-workshops/blob/master/AnomalyDetection-RCF/RandomCutForest.md'>CLICK HERE FOR EXTERNAL STEP BY STEP INSTRUCTIONS</a></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "# Act 1: Investigating the data.  \n",
    "\n",
    "## Step through these steps up until Act 2\n",
    "\n",
    "### Examine the results as we run through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "***\n",
    "\n",
    "Random Cut Forest (RCF) is an algorithm designed to detect anomalous data points within a dataset. \n",
    "\n",
    "Examples of when anomalies are important to detect include when website activity uncharactersitically spikes, when temperature data diverges from a periodic behavior, or when changes to public transit ridership reflect the occurrence of a special event.\n",
    "\n",
    "In this notebook, we will use the SageMaker RCF algorithm to train an RCF model on the Numenta Anomaly Benchmark (NAB) NYC Taxi dataset which records the amount New York City taxi ridership over the course of six months. We will then use this model to predict anomalous events by emitting an \"anomaly score\" for each data point. The main goals of this notebook are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: FILL IN THE BUCKET BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'snively-sagemaker'   # <--- specify a bucket you created in lab1\n",
    "prefix = 'sagemaker/rcf-benchmarks'\n",
    "\n",
    "trainingInstanceType = 'ml.m4.xlarge'\n",
    "inferenceInstanceType = 'ml.m4.xlarge'\n",
    "trainingInstanceCount = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "# check if the bucket exists\n",
    "try:\n",
    "    boto3.Session().client('s3').head_bucket(Bucket=bucket)\n",
    "except botocore.exceptions.ParamValidationError as e:\n",
    "    print('ERROR - You either forgot to specify your S3 bucket or you gave your bucket an invalid name!')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '403':\n",
    "        print(\"ERROR - You don't have permission to access the bucket, {}.\".format(bucket))\n",
    "    elif e.response['Error']['Code'] == '404':\n",
    "        print(\"ERROR - Your bucket, {}, doesn't exist!\".format(bucket))\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    print('Training input/output will be stored in: s3://{}/{}'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "data_filename = 'nyc_taxi.csv'\n",
    "data_source = 'https://raw.githubusercontent.com/bsnively/sagemaker-workshops/master/AnomalyDetection-RCF/nyc_taxi.csv'\n",
    "\n",
    "urllib.request.urlretrieve(data_source, data_filename)\n",
    "taxi_data = pd.read_csv(data_filename, delimiter=',')\n",
    "\n",
    "print(taxi_data.head(20))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams['figure.dpi'] = 100\n",
    "\n",
    "taxi_data.plot()\n",
    "\n",
    "taxi_data[5500:6500].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(taxi_data[5952:6000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "# ACT 2 -- let's train a model.  \n",
    "## this part trains an initial model\n",
    "\n",
    "Examine the code as we run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Particular to a SageMaker RCF training job are the following hyperparameters:\n",
    "\n",
    "* **`num_samples_per_tree`** - the number randomly sampled data points sent to each tree. As a general rule, `1/num_samples_per_tree` should approximate the the estimated ratio of anomalies to normal points in the dataset.\n",
    "* **`num_trees`** - the number of trees to create in the forest. Each tree learns a separate model from different samples of data. The full forest model uses the mean predicted anomaly score from each constituent tree.\n",
    "* **`feature_dim`** - the dimension of each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RandomCutForest\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# specify general training job information\n",
    "rcf = RandomCutForest(role=execution_role,\n",
    "                      train_instance_count=trainingInstanceCount,\n",
    "                      train_instance_type=trainingInstanceType,\n",
    "                      data_location='s3://{}/{}/'.format(bucket, prefix),\n",
    "                      output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                      num_samples_per_tree=512,\n",
    "                      num_trees=50)\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(taxi_data.value.as_matrix().reshape(-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until the cell finishes running,  it often times between 4-8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training job name: {}'.format(rcf.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<hr>\n",
    "<hr>\n",
    "# Act 3: Inferencing (evaluating our model)\n",
    "\n",
    "***\n",
    "\n",
    "A trained Random Cut Forest model does nothing on its own. We now want to use the model we computed to perform inference on data. In this case, it means computing anomaly scores from input time series data points.\n",
    "\n",
    "We create an inference endpoint using the SageMaker Python SDK `deploy()` function from the job we defined above. We specify the instance type where inference is computed as well as an initial number of instances to spin up. We recommend using the `ml.c5` instance type as it provides the fastest inference time at the lowest cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inferenceInstanceType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Endpoint name: {}'.format(rcf_inference.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Serialization/Deserialization\n",
    "\n",
    "We can pass data in a variety of formats to our inference endpoint. In this example we will demonstrate passing CSV-formatted data. Other available formats are JSON-formatted and RecordIO Protobuf. We make use of the SageMaker Python SDK utilities `csv_serializer` and `json_deserializer` when configuring the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "rcf_inference.content_type = 'text/csv'\n",
    "rcf_inference.serializer = csv_serializer\n",
    "rcf_inference.accept = 'application/json'\n",
    "rcf_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data_numpy = taxi_data.value.as_matrix().reshape(-1,1)\n",
    "print(taxi_data_numpy[:6])\n",
    "results = rcf_inference.predict(taxi_data_numpy[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Computing Anomaly Scores\n",
    "\n",
    "Now, let's compute and plot the anomaly scores from the entire taxi dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rcf_inference.predict(taxi_data_numpy)\n",
    "scores = [datum['score'] for datum in results['scores']]\n",
    "\n",
    "# add scores to taxi data frame and print first few values\n",
    "taxi_data['score'] = pd.Series(scores, index=taxi_data.index)\n",
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "#\n",
    "# *Try this out* - change `start` and `end` to zoom in on the \n",
    "# anomaly found earlier in this notebook\n",
    "#\n",
    "start, end = 0, len(taxi_data)\n",
    "#start, end = 5500, 6500\n",
    "taxi_data_subset = taxi_data[start:end]\n",
    "\n",
    "ax1.plot(taxi_data_subset['value'], color='C0', alpha=0.8)\n",
    "ax2.plot(taxi_data_subset['score'], color='C1')\n",
    "\n",
    "ax1.grid(which='major', axis='both')\n",
    "\n",
    "ax1.set_ylabel('Taxi Ridership', color='C0')\n",
    "ax2.set_ylabel('Anomaly Score', color='C1')\n",
    "\n",
    "ax1.tick_params('y', colors='C0')\n",
    "ax2.tick_params('y', colors='C1')\n",
    "\n",
    "ax1.set_ylim(0, 40000)\n",
    "ax2.set_ylim(min(scores), 1.4*max(scores))\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_mean = taxi_data['score'].mean()\n",
    "score_std = taxi_data['score'].std()\n",
    "score_cutoff = score_mean + 3*score_std\n",
    "\n",
    "anomalies = taxi_data_subset[taxi_data_subset['score'] > score_cutoff]\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax2.plot(anomalies.index, anomalies.score, 'ko')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(rcf_inference.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACT 3 -- Getting better results.\n",
    "\n",
    "### Another improvement is make use of a windowing technique called \"shingling\". \n",
    "This is especially useful when working with periodic data with known period, such as the NYC taxi dataset used above. The idea is to treat a period of $P$ datapoints as a single datapoint of feature length $P$ and then run the RCF algorithm on these feature vectors. That is, if our original data consists of points $x_1, x_2, \\ldots, x_N \\in \\mathbb{R}$ then we perform the transformation,\n",
    "\n",
    "```\n",
    "data = [[x_1],            shingled_data = [[x_1, x_2, ..., x_{P}],\n",
    "        [x_2],    --->                     [x_2, x_3, ..., x_{P+1}],\n",
    "        ...                                ...\n",
    "        [x_N]]                             [x_{N-P}, ..., x_{N}]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def shingle(data, shingle_size):\n",
    "    num_data = len(data)\n",
    "    shingled_data = np.zeros((num_data-shingle_size, shingle_size))\n",
    "    \n",
    "    for n in range(num_data - shingle_size):\n",
    "        shingled_data[n] = data[n:(n+shingle_size)]\n",
    "    return shingled_data\n",
    "\n",
    "# single data with shingle size=48 (one day)\n",
    "shingle_size = 48\n",
    "prefix_shingled = prefix + '_shingled'\n",
    "taxi_data_shingled = shingle(taxi_data.values[:,1], shingle_size)\n",
    "print(taxi_data_shingled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new training job and and inference endpoint. (Note that we cannot re-use the endpoint created above because it was trained with one-dimensional data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "\n",
    "# specify general training job information\n",
    "rcf = RandomCutForest(role=execution_role,\n",
    "                      train_instance_count=trainingInstanceCount,\n",
    "                      train_instance_type=trainingInstanceType,\n",
    "                      data_location='s3://{}/{}/'.format(bucket, prefix_shingled),\n",
    "                      output_path='s3://{}/{}/output'.format(bucket, prefix_shingled),\n",
    "                      num_samples_per_tree=512,\n",
    "                      num_trees=50)\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(taxi_data_shingled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inferenceInstanceType,\n",
    ")\n",
    "\n",
    "rcf_inference.content_type = 'text/csv'\n",
    "rcf_inference.serializer = csv_serializer\n",
    "rcf_inference.accept = 'appliation/json'\n",
    "rcf_inference.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above inference endpoint we compute the anomaly scores associated with the shingled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the shingled datapoints\n",
    "results = rcf_inference.predict(taxi_data_shingled)\n",
    "scores = np.array([datum['score'] for datum in results['scores']])\n",
    "\n",
    "# compute the shingled score distribution and cutoff and determine anomalous scores\n",
    "score_mean = scores.mean()\n",
    "score_std = scores.std()\n",
    "score_cutoff = score_mean + 3*score_std\n",
    "\n",
    "anomalies = scores[scores > score_cutoff]\n",
    "anomaly_indices = np.arange(len(scores))[scores > score_cutoff]\n",
    "\n",
    "print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "#\n",
    "# *Try this out* - change `start` and `end` to zoom in on the \n",
    "# anomaly found earlier in this notebook\n",
    "#\n",
    "start, end = 0, len(taxi_data)\n",
    "taxi_data_subset = taxi_data[start:end]\n",
    "\n",
    "ax1.plot(taxi_data['value'], color='C0', alpha=0.8)\n",
    "ax2.plot(scores, color='C1')\n",
    "ax2.scatter(anomaly_indices, anomalies, color='k')\n",
    "\n",
    "ax1.grid(which='major', axis='both')\n",
    "ax1.set_ylabel('Taxi Ridership', color='C0')\n",
    "ax2.set_ylabel('Anomaly Score', color='C1')\n",
    "ax1.tick_params('y', colors='C0')\n",
    "ax2.tick_params('y', colors='C1')\n",
    "ax1.set_ylim(0, 40000)\n",
    "ax2.set_ylim(min(scores), 1.4*max(scores))\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(rcf_inference.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
