{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using DeepAR to forecast electricity time-series data <a name=\"toc\"></a>\n",
    "\n",
    "**Estimated time: 45 min**\n",
    "\n",
    "DeepAR is a supervised learning algorithm for forecasting scalar time series. In this notebook, we will use DeepAR to forecast the electricity demand (across different dimensions) based on time-series of past consumption. \n",
    "\n",
    "This notebook demonstrates how to prepare a dataset of time series for training DeepAR and how to use the trained model for inference. This content was designed to run from within a SageMaker `ml.m4.xlarge` instance.\n",
    "\n",
    "**Table of Contents:** \n",
    "\n",
    "1. [Real Data Set : Electricity Consumption](#data)\n",
    "    1. [Partition the data into training and test sets](#split)\n",
    "    1. [Visualise the time series](#visualize)\n",
    "    1. [S3 Data Formatting](#s3)\n",
    "1. [DeepAR: LSTM & Monte Carlo](#deepar)\n",
    "    1. [Train the model](#fit)\n",
    "    1. [Create endpoint and predictor](#endpoint)\n",
    "1. [Make predictions and plot results](#predict)\n",
    "1. [Delete endpoint](#delete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "whole_start = time.time()\n",
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sagemaker client library for easy interface with sagemaker and s3fs for uploading the training data to S3. \n",
    "\n",
    "(Use `pip` to install missing libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by specifying:\n",
    "- The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these. Here we use the `get_execution_role` function to obtain the role arn which was specified when creating the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = '<your-s3-bucket-name-here>'\n",
    "prefix = 'demos/deepar/forecast-electricity'\n",
    "assert(bucket!='<your-s3-bucket-name-here>')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "s3_data_path = \"{}/{}/data\".format(bucket, prefix)\n",
    "s3_output_path = \"{}/{}/output\".format(bucket, prefix)\n",
    "print('Data location: %s'%s3_data_path)\n",
    "print('Output location: %s'%s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Data Set : Electricity Consumption <a name=\"data\"></a>\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption\n",
    "\n",
    "**Data Set Information:**\n",
    "\n",
    "This archive contains 2075259 measurements gathered between December 2006 and November 2010 (47 months). \n",
    "\n",
    "**Notes:**\n",
    "\n",
    "1. (Global_active_power*1000/60 - Sub_metering_1 - Sub_metering_2 - Sub_metering_3) represents the active energy consumed every minute (in watt hour) in the household by electrical equipment not measured in sub-meterings 1, 2 and 3. \n",
    "\n",
    "2. The dataset contains some missing values in the measurements (nearly 1,25% of the rows). All calendar timestamps are present in the dataset but for some timestamps, the measurement values are missing: a missing value is represented by the absence of value between two consecutive semi-colon attribute separators. For instance, the dataset shows missing values on April 28, 2007.\n",
    "\n",
    "\n",
    "**Attribute Information:**\n",
    "\n",
    "1. `Date`: Date in format dd/mm/yyyy \n",
    "\n",
    "2. `Time`: time in format hh:mm:ss \n",
    "\n",
    "3. `Global_active_power`: household global minute-averaged active power (in kilowatt) \n",
    "\n",
    "4. `Global_reactive_power`: household global minute-averaged reactive power (in kilowatt) \n",
    "\n",
    "5. `Voltage`: minute-averaged voltage (in volt) \n",
    "\n",
    "6. `Global_intensity`: household global minute-averaged current intensity (in ampere) \n",
    "\n",
    "7. `Sub_metering_1`: energy sub-metering No. 1 (in watt-hour of active energy). It corresponds to the `kitchen`, containing mainly a dishwasher, an oven and a microwave (hot plates are not electric but gas powered). \n",
    "\n",
    "8. `Sub_metering_2`: energy sub-metering No. 2 (in watt-hour of active energy). It corresponds to the `laundry room`, containing a washing-machine, a tumble-drier, a refrigerator and a light. \n",
    "\n",
    "9. `Sub_metering_3`: energy sub-metering No. 3 (in watt-hour of active energy). It corresponds to an `electric water-heater` and an `air-conditioner`.\n",
    "\n",
    "\n",
    "<div style=\"text-align: right\"> [Back to top](#toc) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'household_power_consumption.txt'\n",
    "\n",
    "import os\n",
    "if not os.path.isfile(FILENAME):\n",
    "    !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\n",
    "    !unzip household_power_consumption.zip\n",
    "\n",
    "import pandas as pd\n",
    "raw = pd.read_csv(FILENAME, sep=';', parse_dates=True, na_values=['?'])\n",
    "raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the index of the time-series to DatetimeIndex\n",
    "\n",
    "In order for DeepAR to model seasonalities effectively, the algorithm expects the time index in DateTime format. Additionally, all time series in the dataset have to have the same frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "raw['DateTime'] = (raw['Date']+' '+raw['Time']).map(lambda x: datetime.datetime.strptime(x, '%d/%m/%Y %H:%M:%S') )\n",
    "raw.drop(['Date','Time'], axis=1, inplace=True)\n",
    "\n",
    "cols_float = raw.drop('DateTime',axis=1).columns\n",
    "raw[cols_float] = raw[cols_float].apply(lambda x: x.astype(float))\n",
    "raw = raw.set_index('DateTime')\n",
    "\n",
    "print('Percentage of missing data at minute-granularity: %2.2f%%' %(100*raw.isnull().mean().mean()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepAR handles missing data\n",
    "\n",
    "1.25% of the data at minute-granularity is missing. \n",
    "\n",
    "Since minute-forecasts not necessary, we will roll up to hour-granularity. This roll-up incidentally alleviates us of the missing values in our data. However, DeepAR is able to handle missing data: [DeepAR supports missing values, categorical and time-series features, and generalized frequencies](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-deepar-now-supports-missing-values-categorical-and-time-series-features-and-generalized-frequencies/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '2H'\n",
    "df = raw.resample(freq, convention='end').sum()\n",
    "print('Percentage of missing data at hour-granularity:: %2.2f%%' %(100*df.isnull().mean().mean()) )\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partition the data into training and test sets  <a name=\"split\"></a>\n",
    "\n",
    "In this example we want to train a model that can predict the next `1` week of electricity  time series usage. Since our time series has hourly granularity, 1 week is equivalent to 168 hours.\n",
    "\n",
    "We also need to configure the so-called `context_length`, which determines how much context of the time series the model should take into account when making the prediction, i.e. how many previous points to look at. A typical value to start with is around the same size as the `prediction_length`. In practice, it often beneficial to use a longer `context_length`. **NB:** In addition to the `context_length` the model also takes into account the values of the time series at typical seasonal windows e.g. for hourly data the model will look at the value of the series 24h ago, one week ago one month ago etc. So it is not necessary to make the `context_length` span an entire month if you expect monthly seasonalities in your hourly data.\n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_length = int(7*24/int(freq.replace('H','')))\n",
    "context_length = int(7*24/int(freq.replace('H','')))\n",
    "\n",
    "print('Preditction length: %i %s' %(prediction_length, freq))\n",
    "print('Context length: %i %s' %(context_length, freq))\n",
    "print('-->  1 week')\n",
    "\n",
    "n_weeks = 6\n",
    "end_training = df.index[-n_weeks*prediction_length]\n",
    "\n",
    "time_series = []\n",
    "for ts in df.columns:\n",
    "    time_series.append(df[ts])\n",
    "    \n",
    "time_series_training = []\n",
    "for ts in df.columns:\n",
    "    time_series_training.append(df.loc[:end_training][ts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the time series <a name=\"visualize\"></a>\n",
    "\n",
    "Often one is interested in tuning or evaluating the model by looking at error metrics on a hold-out set. For other machine learning tasks such as classification, one typically does this by randomly separating examples into train/test sets. For forecasting it is important to do this train/test split in time rather than by series.\n",
    "\n",
    "<div style=\"text-align: right\"> [Back to top](#toc) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('Spectral')\n",
    "colors = cmap(np.arange(0,len(cols_float))/len(cols_float))\n",
    "\n",
    "\n",
    "plt.figure(figsize=[14,8]);\n",
    "for c in range(len(cols_float)):\n",
    "    plt.plot(df.loc[:end_training][cols_float[c]], alpha=0.5, color=colors[c], label=cols_float[c]);  \n",
    "plt.legend(loc='center left');\n",
    "for c in range(len(cols_float)):\n",
    "    plt.plot(df.loc[end_training:][cols_float[c]], alpha=0.25, color=colors[c], label=None);\n",
    "    #plt.plot(df[cols_float[c]], alpha=0.25, color=colors[c], linestyle=':', label=None);\n",
    "plt.axvline(x=end_training, color='k', linestyle=':');\n",
    "plt.text(df.index[int((df.shape[0]-n_weeks*prediction_length)*0.75)], df.max().max()/2, 'Train');\n",
    "plt.text(df.index[df.shape[0]-int(n_weeks*prediction_length/2)], df.max().max()/2, 'Test');\n",
    "plt.xlabel('Time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3 Data Formatting <a name=\"s3\"></a>\n",
    "\n",
    "DeepAR supports json, gzipped jsonlines, and parquet. Here we will go with json strings; the following utility functions convert `pandas.Series` objects into the JSON format that DeepAR can expects. We shall use these to write the data to S3.\n",
    "\n",
    "<div style=\"text-align: right\"> [Back to top](#toc) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))\n",
    "\n",
    "encoding = \"utf-8\"\n",
    "s3filesystem = s3fs.S3FileSystem()\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'wb') as fp:\n",
    "    for ts in time_series_training:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))\n",
    "\n",
    "with s3filesystem.open(s3_data_path + \"/test/test.json\", 'wb') as fp:\n",
    "    for ts in time_series:\n",
    "        fp.write(series_to_jsonline(ts).encode(encoding))\n",
    "        fp.write('\\n'.encode(encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepAR: LSTM & Monte Carlo\n",
    "\n",
    "Amazon SageMaker's DeepAR is a methodology for producing accurate probabilistic forecasts, based on training an auto-regressive recurrent network model on a large number of related time series.  DeepAR produces more accurate forecasts than other state-of-the-art methods, while requiring minimal manual work.\n",
    "\n",
    "* The DeepAR algorithm first tailors a `Long-Short Term Memory` (LSTM)-based recurrent neural network architecture to the data. DeepAR then produces probabilistic forecasts in the form of `Monte Carlo` simulation. \n",
    "* `Monte Carlo` samples are empirically generated pseudo-observations that can be used to compute consistent quantile estimates for all sub-ranges in the prediction horizon.\n",
    "* DeepAR also uses item-similarity to handle the `Cold Start` problem, which is to make predictions for items with little or no history at all.\n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model <a name=\"fit\"></a>\n",
    "\n",
    "Next, we configure the container image to be used for the region in which our notebook is running, and can now define the estimator that will launch the training job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n",
    "\n",
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='DeepAR-forecast-electricity',\n",
    "    output_path=\"s3://\" + s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set some hyperparameters: \n",
    "\n",
    "\n",
    "| Hyperparameters |  Value |  \n",
    "|---|---|\n",
    "|  epochs | 100  |   \n",
    "|  time granularity | bi-hourly  |   \n",
    "|  domain |  $\\mathbb{R}^+,\\mathbb{N}$ |   \n",
    "|  number training examples |  8606 |   \n",
    "|  batch size | 32  |   \n",
    "|  learning rate | 1e-3  |   \n",
    "|  LSTM layers | 3  |   \n",
    "|  LSTM nodes | 40  |   \n",
    "|  likelihood | gaussian  |   \n",
    "\n",
    "Refer to the documentation for a full description of the available parameters: https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html\n",
    "\n",
    "For example, we elect the Gaussian likelihood because we have real-valued data. Other likelihood models can also readily be used, as long as samples from the distribution can cheaply be obtained, and the log-likelihood and its gradients wrt. the parameters can be evaluated. e.g.  \n",
    "- **gaussian:** Use for real-valued data.\n",
    "\n",
    "- **beta:** Use for real-valued targets between 0 and 1 inclusive.\n",
    "\n",
    "- **negative-binomial:** Use for count data (non-negative integers).\n",
    "\n",
    "- **student-T:** An alternative for real-valued data that works well for bursty data.\n",
    "\n",
    "- **deterministic-L1:** A loss function that does not estimate uncertainty and only learns a point forecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"100\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}\n",
    "\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to launch the training job. SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel, as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test data set. This is done by predicting the last `prediction_length` points of each time series in the test set and comparing this to the actual value of the time series. The computed error metrics will be included as part of the log output.\n",
    "\n",
    "**Note:** The next cell may takes **4 min** to complete. You'll notice at the end that the `billable time` is only around ~150seconds. This is because one only pays for consumption of the resources, not the provisioning time.\n",
    "\n",
    "You can follow the progress of the training job, either in the console at [AWS SageMaker Training Jobs Console](https://console.aws.amazon.com/sagemaker/home?#/jobs), or in print out here in the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": \"s3://{}/train/\".format(s3_data_path),\n",
    "    \"test\": \"s3://{}/test/\".format(s3_data_path)\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint and predictor <a name=\"endpoint\"></a>\n",
    "\n",
    "Now that we have trained a model, we can use it to perform predictions by deploying it to an endpoint. First we want to define a [sagemaker.predictor.RealTimeEndpoint](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/predictor.py) to give our endpoint more functionality and flexibility. This is the DeepARPredictor predictor class below will allow us:\n",
    "\n",
    "* To query the endpoint and perform predictions\n",
    "* To make requests using `pandas.Series` objects rather than raw JSON strings.\n",
    "* To ask for specific quantiles\n",
    "* The option of returning the Monte Carlo samples or not\n",
    "\n",
    "<div style=\"text-align: right\"> [Back to top](#toc) </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_JSON, **kwargs)\n",
    "        \n",
    "    def predict(self, ts, cat=None, dynamic_feat=None, \n",
    "                num_samples=100, return_samples=False, quantiles=[\"0.1\", \"0.5\", \"0.9\"]):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "        \n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "        \n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + 1\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "    \n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None)\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n",
    "    \n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode('utf-8'))['predictions'][0]\n",
    "        prediction_length = len(next(iter(predictions['quantiles'].values())))\n",
    "        prediction_index = pd.DatetimeIndex(start=prediction_time, freq=freq, periods=prediction_length)        \n",
    "        if return_samples:\n",
    "            dict_of_samples = {'sample_' + str(i): s for i, s in enumerate(predictions['samples'])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(data={**predictions['quantiles'], **dict_of_samples}, index=prediction_index)\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]        \n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat        \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to deploy our endpoint.\n",
    "\n",
    "**Note:** This endpoint will take approximately 5 minutes to spin up. Execute the cell below and use the 5 minutes to continue reading below.\n",
    "\n",
    "It is important to delete an endpoint once it is no longer needed; a cell at the very bottom of this notebook gives the code to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    predictor_cls=DeepARPredictor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .... To read while the endpoint is deploying....\n",
    "\n",
    "Let’s elaborate on the DeepAR model's architecture by walking through an example. When interested in quantifying the confidence of the estimates produced, then it's probabilistic forecasts that are wanted. The data we’re working with is real-valued, so let’s opt for the Gaussian likelihood:\n",
    "$$\\ell(y_t|\\mu_t,\\sigma_t)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp{\\frac{-(y_t-\\mu_t)^2}{2\\sigma^2}}.$$\n",
    "\n",
    "$\\theta$ represents the `parameters of the likelihood`. In the case of Gaussian, $\\theta_t$ will represent the mean and standard deviation:  $$\\theta_t = \\{\\mu_{t},\\sigma_{t}\\}.$$\n",
    "\n",
    "The neural network’s last hidden layer results in $h_{d,t}$. This $h_{d,t}$ will undergo 1 activation function per likelihood parameter. For example, for the Gaussian likelihood, $h_{d,t}$ is transformed by an affine activation function to get the mean:\n",
    "$$\\mu_{t} = w_{\\mu}^T h_{d,t} + b_{\\mu},$$\n",
    "and then $h$ is transformed by a softplus activation to get the standard deviation:\n",
    "$$\\sigma_t = \\log\\left(1 + \\exp(w_{\\sigma}^T h_{d,t} + b_{\\sigma})\\right).$$\n",
    "\n",
    "The `activation parameters` are the $w_{\\mu},b_{\\mu},w_{\\sigma},b_{\\sigma}$ parameters within the activation functions. The NN is trained to learn the fixed constants of the activation parameters.  Since the $h_{d,t}$ output  vary given each time-step's input, this still allows the likelihood parameters to vary over time, and therefore capture dynamic behaviors in the time-series data.\n",
    "\n",
    "![DeepAR Training](deepar_training.png)\n",
    "\n",
    "From the above diagram, the <span style=\"color:green\">green</span> input at each time-step is the data point preceding the current time-step’s data, as well as the previous network’s output. For simplicity, on this diagram we aren’t showing covariates which would also be input.\n",
    "\n",
    "The LSTM layers are shown in <span style=\"color:red\">red</span>, and the final hidden layer produces the $h_{d,t}$ value, which we saw in the previous slide will undergo an activation function for each parameter of the specified likelihood. To learn the activation function parameters, the NN takes the $h_{d,t}$ at time $t$ and the data up until time $t$, and performs Stochastic Gradient Descent (SGD) to yield the activation parameters which maximize the likelihood at time $t$. The <span style=\"color:blue\">blue</span> output layer uses the SGD-optimized activation functions to output the maximum likelihood parameters.\n",
    "\n",
    "This is how DeepAR trains its model to your data input. Now we want to DeepAR to give us probabilistic forecasts for the next time-step.\n",
    "\n",
    "![DeepAR Forecast](deepar_forecast.png)\n",
    "\n",
    "The <span style=\"color:magenta\">pink</span> line marks our current point in time, divides our training data from data not yet seen. For the first input, it can use the data point of the current time. The input will be processed by the trained LSTM layers, and subsequently get activated by the optimized activation functions to output the maximum-likelihood theta parameters at time $t+1$. \n",
    "\n",
    "Now that DeepAR has completed the likelihood with its parameter estimates, DeepAR can simulate `Monte Carlo (MC) samples` from this likelihood and produce an empirical distribution for the predicted datapoint - the probabilistic forecasts shown in <span style=\"color:purple\">purple</span>. The MC samples produced at time $t+1$ are used as input for time $t+2$, etc, until the end of the prediction horizon. In the interactive plots below, we'll see how Monte Carlo samples are able to provide us a confidence interval about the point estimate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Make predictions and plot results <a name=\"predict\"></a>\n",
    "\n",
    "Once the endpoint finishes deploying, we can use the previously created `predictor` object.  First, let's see its output of quantile-predictions for the time-series Global_active_power.\n",
    "\n",
    "<div style=\"text-align: right\"><a href=\"#toc\">Back to top</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(ts=df.loc[end_training:, 'Global_active_power'], quantiles=[0.10, 0.5, 0.90], num_samples=100).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[end_training:, 'Global_active_power'].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see compare the results with the actual data we kept in the test set. We include a plot function to visualize the predictions against the actual targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(\n",
    "    predictor, \n",
    "    target_ts, \n",
    "    cat=None, \n",
    "    dynamic_feat=None, \n",
    "    forecast_date=end_training, \n",
    "    show_samples=False, \n",
    "    plot_history=7 * 12,\n",
    "    confidence=80,\n",
    "    num_samples=100\n",
    "):\n",
    "    print(\"Calling endpoint to generate {} predictions starting from {} ...\".format(target_ts.name, str(forecast_date)))\n",
    "    assert(confidence > 50 and confidence < 100)\n",
    "    low_quantile = 0.5 - confidence * 0.005\n",
    "    up_quantile = confidence * 0.005 + 0.5\n",
    "        \n",
    "    # we first construct the argument to call our model\n",
    "    args = {\n",
    "        \"ts\": target_ts[:forecast_date],\n",
    "        \"return_samples\": show_samples,\n",
    "        \"quantiles\": [low_quantile, 0.5, up_quantile],\n",
    "        \"num_samples\": num_samples\n",
    "    }\n",
    "\n",
    "\n",
    "    if dynamic_feat is not None:\n",
    "        args[\"dynamic_feat\"] = dynamic_feat\n",
    "        fig = plt.figure(figsize=(20, 6))\n",
    "        ax = plt.subplot(2, 1, 1)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20, 3))\n",
    "        ax = plt.subplot(1,1,1)\n",
    "    \n",
    "    if cat is not None:\n",
    "        args[\"cat\"] = cat\n",
    "        ax.text(0.9, 0.9, 'cat = {}'.format(cat), transform=ax.transAxes)\n",
    "\n",
    "    # call the end point to get the prediction\n",
    "    prediction = predictor.predict(**args)\n",
    "\n",
    "    # plot the samples\n",
    "    mccolor = 'blue'\n",
    "    if show_samples: \n",
    "        for key in prediction.keys():\n",
    "            if \"sample\" in key:\n",
    "                prediction[key].plot(color='lightskyblue', alpha=0.2, label='_nolegend_')\n",
    "                #prediction[key].plot(color='light'+mccolor.replace('dark',''), alpha=0.2, label='_nolegend_')\n",
    "                \n",
    "                \n",
    "    # plot the target\n",
    "    target_section = target_ts[forecast_date-plot_history:forecast_date+prediction_length]\n",
    "    target_section.plot(color=\"black\", label='target')\n",
    "    plt.title(target_ts.name.upper(), color='darkred')\n",
    "    \n",
    "    # plot the confidence interval and the median predicted\n",
    "    ax.fill_between(\n",
    "        prediction[str(low_quantile)].index, \n",
    "        prediction[str(low_quantile)].values, \n",
    "        prediction[str(up_quantile)].values, \n",
    "        color=mccolor, alpha=0.3, label='{}% confidence interval'.format(confidence)\n",
    "    )\n",
    "    prediction[\"0.5\"].plot(color=mccolor, label='P50')\n",
    "    ax.legend(loc=2)    \n",
    "    \n",
    "    # fix the scale as the samples may change it\n",
    "    ax.set_ylim(target_section.min() * 0.5, target_section.max() * 1.5)\n",
    "    \n",
    "    if dynamic_feat is not None:\n",
    "        for i, f in enumerate(dynamic_feat, start=1):\n",
    "            ax = plt.subplot(len(dynamic_feat) * 2, 1, len(dynamic_feat) + i, sharex=ax)\n",
    "            feat_ts = pd.Series(\n",
    "                index=pd.DatetimeIndex(start=target_ts.index[0], freq=target_ts.index.freq, periods=len(f)),\n",
    "                data=f\n",
    "            )\n",
    "            feat_ts[forecast_date-plot_history:forecast_date+prediction_length].plot(ax=ax, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an interactive plot, where you can explore different confidence intervals, forecast days, and Monte Carlo samples. For completeness, the Monte Carlo probabilistic forecasts for all seven time-series are plotted farther below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "@interact_manual(\n",
    "    series_id=IntSlider(min=0, max=len(time_series_training)-1, value=6, style=style), \n",
    "    forecast_day=IntSlider(min=0, max=100, value=0, style=style),\n",
    "    confidence=IntSlider(min=60, max=95, value=80, step=5, style=style),\n",
    "    history_weeks_plot=IntSlider(min=1, max=20, value=1, style=style),\n",
    "    num_samples=IntSlider(min=100, max=1000, value=100, step=500, style=style),\n",
    "    show_samples=Checkbox(value=True),\n",
    "    continuous_update=False\n",
    ")\n",
    "def plot_interact(series_id, forecast_day, confidence, history_weeks_plot, show_samples, num_samples):\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=time_series[series_id],\n",
    "        forecast_date=end_training + datetime.timedelta(days=forecast_day),\n",
    "        show_samples=show_samples,\n",
    "        plot_history=history_weeks_plot * prediction_length,\n",
    "        confidence=confidence,\n",
    "        num_samples=num_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(time_series_training)):\n",
    "    plt.figure()\n",
    "    plot(\n",
    "        predictor,\n",
    "        target_ts=time_series[i],\n",
    "        forecast_date=end_training,\n",
    "        show_samples=True,\n",
    "        plot_history= 2 * prediction_length,\n",
    "        confidence=60,\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete endpoint <a name=\"delete\"></a>\n",
    "\n",
    "Remember to delete an endpoint once it is no longer needed. \n",
    "\n",
    "[Back to top](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sagemaker_session.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#whole_end = time.time()\n",
    "#print('Whole notebook elapsed time (min):', (whole_end-whole_start)/60.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thank you\n",
    "\n",
    "<div style=\"text-align: right\"> END. OF. LAB. </div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p36",
   "language": "python",
   "name": "conda_chainer_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
